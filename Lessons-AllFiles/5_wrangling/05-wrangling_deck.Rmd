---
title: "Module 3: R"
subtitle: "Wrangling"
author:
- "Instructor: Anjali Silva, PhD"
institute: "Data Sciences Institute, University of Toronto"
date: "2022"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono"),
  colors = c(
    red = "#f34213",
    purple = "#3e2f5b",
    orange = "#ff8811",
    green = "#136f63",
    white = "#FFFFFF"
  )
)

```

```{r, include=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(janitor)
library(kableExtra)
library(rgdal)
library(data.table)
```

# Course Documents
* Visit: https://github.com/anjalisilva/IntroductionToR 

* All course material will be available via IntroductionToR GitHub repository (https://github.com/anjalisilva/IntroductionToR). Folder structure is as follows:
   * Lessons - All files: This folder contains all files.
   * **Lessons - Data only**: This folder contains data only.
   * **Lessons - Lesson Plans only**: This folder contains lesson plans only.
   * **Lessons - PDF only**: This folder contains slide PDFs only.
   * README - README file
   * .gitignore - Files to ignore specified by instructor

## Course Contacts
* Instructor: Anjali Silva
  Email: a.silva@utoronto.ca (Must use the subject line DSI-IntroR.   E.g., DSI-IntroR: Inquiry about Lecture I.)

* TA: see GitHub

---

# Overview

- Importing data (Wickham and Grolemund, 2017, Chapter 11; Timbers et al. 2021, Chapter 2) & Saving data
- Pivot (Wickham and Grolemund, 2017, Chapter 12; Timbers et al. 2021, Chapter 3.4)
- Joining data (Wickham and Grolemund, 2017, Chapter 13)
- data.table (Wiley and Wiley, 2020, Chapter 7; https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html)

---

class: inverse, center, middle

# Data Import

---

# Packages and functions for reading in different file types

CSV files

```{r, eval=FALSE}
readr::read_csv()
```

Excel workbooks

```{r, eval=FALSE}
readxl::read_excel()
```

---
# Packages and functions for reading in different file types

SPSS files

```{r, eval=FALSE}
haven::read_sav()
```

Stata files

```{r, eval=FALSE}
haven::read_dta()
```

SAS files

```{r, eval=FALSE}
haven::read_sas()
```

---

## Specifications for import

When the separator isn't a comma:

```{r, eval=F}
read_csv("mydata.csv", sep = ";")
```

When the first two lines are metadata:

```{r, eval=F}
read_csv("mydata.csv", skip = 2)
```

When there are no column names:

```{r, eval=F}
read_csv("mydata.csv", col_names = FALSE)
```

---

## Import issues

The reader function guesses what data type each column is. This might not always work the way you want. You can specify column types if necessary:

```{r, eval=F}
read_csv("mydata.csv",
         col_types = cols(.default = col_character())
```

---

## Writing data

```{r, eval=F}
write_csv(mydata, "mydata.csv")
```

---

class: inverse, center, middle

# Pivot

---

## Tidy data

Rules for tidy data:

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

Pivoting will help you get to tidy data form.

---
## Tidy data

Let's say we have a dataset with the weights of two pet cats:

```{r}
wide_data <- tibble(
  year = c(2017, 2018, 2019, 2020, 2021, 2022),
  milo = c(4, 6.3, 8, 7.9, 8.1, 8.1),
  kitty = c(15.6, 15.9, 14, 12.2, 10.9, 9.9)
)

wide_data
```

---
## Tidy data

To make the data tidy, we will use a function called pivot_longer:

```{r}
long_data <- wide_data %>%
  pivot_longer(cols = c("milo", "kitty"),
               names_to = "cat",
               values_to = "weight")
long_data
```

---
## Tidy data

You can also reverse:

```{r}
long_data %>%
  pivot_wider(names_from = "cat",
              values_from = "weight")
```

This may be useful when making a summary table to display.

---

class: inverse, center, middle

# Join

---

# Mutating joins

Joins combine tables based on an identified key, usually a variable that the two tables share in common.

```{r, eval=FALSE}
left_join(), right_join(), full_join(), inner_join()
```

```{r echo=FALSE, out.width= 2000}
knitr::include_graphics("join-venn.png")
```

---

## Keys

1. Primary keys uniquely identify an observation in its table
2. Foreign keys uniquely identify an observation in another table

Primary keys may match foreign keys, if the variable is present in both tables and has the same name.

Primary and foreign keys have a specific relation: it could be one-to-one, or one-to-many.

---

# Example using toy data

We will make two small datasets, each with year as a variable. Year will be the key we use to join.

```{r}
employment <- tibble(year = c(1990, 1991, 1992, 1994),
                     rate = c(.05, .02, .04, .02))
employment
```

---
# Example using toy data

```{r}
housing <- tibble(date = c(1991, 1992, 1993, 1994, 1995),
                  rate = c(0.89, 0.6, 0.75, 0.88, 0.9))
housing
```

---

# Inner Join

The rows correspond to years that are present in both `employment` and `housing`:

```{r}
employment %>%
  inner_join(housing, by = c("year" = "date"))
``` 

Because both tables have a variable named `rate`, the joined table has columns named `rate.x` from the left table (employment) and `rate.y` from the right table (housing).

---

# Left Join

The rows correspond to years that are present in both `employment`but not necessarily `housing`:

```{r}
employment %>%
  left_join(housing, by = c("year" = "date"))
``` 

Missing values are filled with `NA`.

---

# Right Join

The rows correspond to years that are present in `housing` but not necessarily `employment`:

```{r}
employment %>%
  right_join(housing, by = c("year" = "date"))
``` 

---

# Full Join

The rows correspond to years that are present in `employment`or `housing`:

```{r}
employment %>%
  full_join(housing, by = c("year" = "date"))
``` 

---

# Filtering joins

Semi joins keep all observations in the employment table that have a match in the housing table:

```{r}
employment %>%
  semi_join(housing, by = c("year" = "date"))
```

---
# Filtering joins

Anti joins drop all observations in employment that have a match in housing:

```{r}
employment %>%
  anti_join(housing, by = c("year" = "date"))
```

---

class: inverse, center, middle

# data.table

---

## Introduction

data.tables are faster and more memory efficient than data.frames, making them better suited for large operations on large data sets.

They are keyed, which makes it possible to use binary search. A key is an index, created from column(s) in the data.table. It may or may not be unique.

---
# data.table

```{r}
data(iris)

dt_iris <- as.data.table(iris)

dt_iris
```

---
# data.table
You can see how many tables are stored and what size they are:

```{r}
tables()
```

---
# data.table
To get information about our data, we can use sapply and class:

```{r}
sapply(dt_iris, class)
```

---
# data.table
To check if the data.table has a key:

```{r}
haskey(dt_iris)
```

To set a key:

```{r}
setkey(dt_iris, Species)
haskey(dt_iris)
```

---

# General form

datatable[i, j, k]

Where:

- from datatable
- i is selected and/or filtered
- j is calculated
- k is grouped by

---

## Subsetting rows

```{r eval=FALSE}
dt_iris[Petal.Width > 0.5]
```

---

## Selecting columns

Selecting a column to return as a vector:

```{r eval=FALSE}
dt_iris[, Species]
```

---
## Selecting columns

Selecting a column to return as a data.table:

```{r eval=FALSE}
dt_iris[, list(Species)]
```

---

## Computations

Counting the number of cases where Petal.Length plus Petal.Width is greater than 2:

```{r eval=FALSE}
dt_iris[, sum((Petal.Length + Petal.Width) > 2)]
```

---

## Combining subsetting and computation

Counting the number of cases where Species is "setosa" and Petal.Length plus Petal.Width is greater than 2:

```{r eval=FALSE}
dt_iris[Species == "setosa", sum((Petal.Length + Petal.Width) > 2)]
```

---

## Counting observations in current group

```{r eval=FALSE}
dt_iris[Species == "setosa", .N]
```

---

## Aggregations

Counting the number in each Species group:

```{r eval=FALSE}
dt_iris[, .(.N), by = .(Species)]
```

---

## Sorting

```{r eval=FALSE}
dt_iris[Petal.Width > 1, .(mean(Petal.Length)), keyby = .(Species)]
```

---

## Ordering

Ordering by Species and then by reverse Petal.Width:

```{r eval=FALSE}
dt_iris[order(Species, -Petal.Width)]
```

---

## Grouping by expression rather than column

Grouping those that have Petal.Length greater/not than 4, and Petal.Width greater/not than 1:

```{r eval=FALSE}
dt_iris[, .N, .(Petal.Length > 4, Petal.Width > 1)]
```

---

## Computations for multiple columns

Taking the mean of all columns (except the grouping column, Species) using lapply and .SD:

```{r eval=FALSE}
dt_iris[, lapply(.SD, mean), by = Species]
```

---

## Subsetting for each group

Getting the first two rows for all columns, for each Species:

```{r eval=FALSE}
dt_iris[, head(.SD, 2), by = Species]
```

---

class: inverse, center, middle

# Exercises

---
# Exercises
1 - Tidy the data below:

```{r}
data <- tibble(
  group = c("treat", "control"),
  survival = c(17, 11),
  deceased = c(3, 9)
)
```

---
# Exercises
2 - Join the dataset flights to the dataset airlines. What should the key(s) be? What do the different types of joins look like?

```{r}
library(nycflights13)
data("flights")
data("airlines")
```

---
# Exercises
3 - Using flights and data.table, group based on cases that have dep_delay < 0 and those that have arr_delay > 0 and count the number in each group. How many groups/rows are there? How many in each group?

---

class: inverse, center, middle

# Any questions?

